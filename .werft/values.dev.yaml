installation:
  stage: devstaging
  tenant: gitpod-core
  region: europe-west1
  cluster: "00"
  shortname: "dev"
hostname: staging.gitpod-dev.com
imagePrefix: eu.gcr.io/gitpod-core-dev/build/
certificatesSecret:
  secretName: proxy-config-certificates
version: not-set
imagePullPolicy: Always
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: gitpod.io/workload_meta
          operator: In
          values:
          - "true"
authProviders: []
tracing:
  endoint: http://jaeger-collector:14268/api/traces
  samplerType: const
  samplerParam: "1"

# we hit the "max. 110 pods/node" situation pretty often with our current core-dev setup.
# the proper way to fix those would be to adjust the CIDR for workload NodePool which is a bit more work.
# as a workaround, we blow up our RAM requests to trigger scaleup earlier.
# Note: This only works because we tune down our DaemonSet's requests to near-0 (because DS pods don't trigger scalups!)
resources:
  default:
    # as opposed to 200Mi, the default
    # we make static pods big enough so that 100 pods fill up the whole node (we ignore other DaemonSets here because they are quite small),
    # and assume not all envs carry workspaces all the time:
    # => 32Gi / 100 ~ 328Mi => 350Mi
    memory: 350Mi

components:

  agentSmith:
    name: "agent-smith"
    disabled: false
    # in preview envs, we never want DaemonSets not to be scheduled (because they don't trigger scaleup)
    resources:
      cpu: 1m
      memory: 1Mi

  server:
    replicas: 1
    makeNewUsersAdmin: true # for development
    theiaPluginsBucketName: gitpod-core-dev-plugins
    enableLocalApp: true
    enableOAuthServer: true
    blockNewUsers: true
    blockNewUsersPasslist:
    - "gitpod.io"
    resources:
      # in preview envs, we want deployments to push scale-up early
      memory: 350Mi

  registryFacade:
    daemonSet: true
    # in preview envs, we never want DaemonSets not to be scheduled (because they don't trigger scaleup)
    resources:
      cpu: 1m
      memory: 1Mi

  contentService:
    remoteStorage:
      blobQuota: 1073741824 # 1 GiB
    resources:
      # in preview envs, we want deployments to push scale-up early
      memory: 350Mi

  workspace:
    # configure GCP registry
    pullSecret:
      secretName: gcp-sa-registry-auth
    affinity:
      default: "gitpod.io/workload_workspace"
    templates:
      default:
        spec:
          dnsConfig:
            nameservers:
            - 1.1.1.1
            - 8.8.8.8
          dnsPolicy: None   # do NOT query against K8s DNS (https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/)
          env:
          - name: THEIA_PREVENT_METADATA_ACCESS
            value: true
      regular:
        spec:
          containers:
          - name: "workspace"
            env:
            - name: THEIA_RATELIMIT_LOG
              value: "50"
            - name: SUPERVISOR_DEBUG_ENABLE
              value: "true"
      prebuild:
        spec:
          containers:
          - name: workspace
            # Intended to reduce the density for prebuilds
            resources:
              limits:
                cpu: "5"
                memory: 12Gi
              requests:
                cpu: 1m
                ephemeral-storage: 5Gi
                memory: 4608Mi  # = 2 * 2304Mi

  # Allow per-branch ingress from another, in-cluster proxy
  proxy:
    replicas: 1
    ports:
      http:
        expose: true
        containerPort: 80
        nodePort: null
      https:
        expose: true
        containerPort: 443
        nodePort: null
      metrics:
        containerPort: 9145
    serviceSessionAffinity: "None"
    serviceExternalTrafficPolicy: null
    serviceType: "ClusterIP"
    deployIngressService: false
    loadBalancerIP: null
    resources:
      # in preview envs, we want deployments to push scale-up early
      memory: 350Mi

  # Enable events trace
  wsManager:
    eventTraceLogLocation: "/tmp/evts.json"
    resources:
      # in preview envs, we want deployments to push scale-up early
      memory: 350Mi

  imageBuilder:
    hostDindData: "/mnt/disks/ssd0/builder"
    # configure GCP registry
    registry:
      name: eu.gcr.io/gitpod-core-dev/registry
      secretName: gcp-sa-registry-auth
      path: gcp-sa-registry-auth
    registryCerts: []
    resources:
      # in preview envs, we want deployments to push scale-up early
      memory: 350Mi

  wsDaemon:
    hostWorkspaceArea: /mnt/disks/ssd0/workspaces
    volumes:
    - name: gcloud-tmp
      hostPath:
        path: /mnt/disks/ssd0/sync-tmp
        type: DirectoryOrCreate
    volumeMounts:
    - mountPath: /mnt/sync-tmp
      name: gcloud-tmp
    userNamespaces:
      fsShift: shiftfs
      shiftfsModuleLoader:
        enabled: true
      seccompProfileInstaller:
        enabled: true
    # in preview envs, we never want DaemonSets not to be scheduled (because they don't trigger scaleup)
    resources:
      cpu: 1m
      memory: 1Mi
    nodeRoots:
      - /var/lib
      - /run/containerd/io.containerd.runtime.v2.task/k8s.io

  wsScheduler:
    scaler:
      enabled: true
      controller:
        kind: "constant"
        constant:
          setpoint: 1
    resources:
      # in preview envs, we want deployments to push scale-up early
      memory: 350Mi

  # Enable ws-proxy in dev
  wsProxy:
    name: "ws-proxy"
    disabled: false
    replicas: 1
    wsManagerProxy:
      enabled: true
    ports:
      wsManagerProxy:
        expose: true
        containerPort: 8081
    resources:
      # in preview envs, we want deployments to push scale-up early
      memory: 350Mi

# configure GCP registry
docker-registry:
  enabled: false

minio:
  accessKey: EXAMPLEvalue
  secretKey: Someone.Should/ReallyChangeThisKey!!
  serviceAccount:
    name: ws-daemon
    create: false
  # make sure the pod ends up where it's supposed to stay
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: dev/workload
            operator: In
            values:
            - "workload"
  resources:
    requests:
      # in preview envs, we want deployments to push scale-up early
      memory: 350Mi

mysql:
  primary:
    # make sure the pod ends up where it's supposed to stay
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: dev/workload
              operator: In
              values:
              - "workload"
    resources:
      requests:
        # in preview envs, we want deployments to push scale-up early
        memory: 350Mi

rabbitmq:
  # ensure shovels are configured on boot
  shovels:
    - name: messagebus-0
      srcUri: "amqp://$USERNAME:$PASSWORD@messagebus-0"
  auth:
    username: override-me
    password: override-me
  # make sure the pod ends up where it's supposed to stay
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: dev/workload
            operator: In
            values:
            - "workload"
  resources:
    requests:
      # in preview envs, we want deployments to push scale-up early
      memory: 350Mi

cert-manager:
  enabled: true
